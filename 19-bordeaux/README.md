[Rencontre scientifique](https://sygefor.reseau-urfist.fr/#/training/7705/8525/7338a781b57bf45847c702bfd05b2a80) :
Repenser la robustesse et la fiabilité en recherche : les chercheurs face à la crise de la reproductibilité

# A simple cure to the p < 0.05 disease

In this talk, I will argue that there is no replicability crisis, just a collection of bad habits and wrong expectations from all involved in the research enterprise. The main habit is the use of P values and confidence intervals to dichotomise results as “significant” or “not significant”. This habit stems from poor training in statistics and unrealistic expectations about our research methods. The cure to the dichotomy madness involves important steps:
- to provide clear definitions;
- to embrace measurement uncertainty;
- to understand the difficulty in assessing theories;
- to accept that no single experiment can establish a new phenomenon;
- to match statistical tools to empirical questions, instead of following the herd.  

The R code for the numerical examples in the talk are provided [here](docs/examples.md).

Other examples are from these blog posts:

- [Small n correlations cannot be trusted](https://garstats.wordpress.com/2018/06/01/smallncorr/)

- [Small n correlations + p values = disaster](https://garstats.wordpress.com/2018/06/22/corrcondpval/)

- [What can we learn from 10,000 experiments?](https://garstats.wordpress.com/2018/01/24/10000/)

- [Hierarchical shift function: a powerful alternative to the t-test](https://garstats.wordpress.com/2019/02/21/hsf/)

From a [workshop](https://github.com/GRousselet/teaching/tree/master/19-bordeaux) on robust statistics given in Bordeaux.

From the `rogme` R package [documentation](https://github.com/GRousselet/rogme). 